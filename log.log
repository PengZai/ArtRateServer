nohup: ignoring input
[2024-12-06 11:43:17 +0800] [4004264] [INFO] Starting gunicorn 20.1.0
[2024-12-06 11:43:17 +0800] [4004264] [INFO] Listening at: http://10.206.0.13:8001 (4004264)
[2024-12-06 11:43:17 +0800] [4004264] [INFO] Using worker: gthread
[2024-12-06 11:43:17 +0800] [4004274] [INFO] Booting worker with pid: 4004274
[2024-12-06 11:43:17 +0800] [4004275] [INFO] Booting worker with pid: 4004275
Some weights of the model checkpoint at /var/www/html/ArtRate/ArtRate_server/art/art_assessment_model/microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /var/www/html/ArtRate/ArtRate_server/art/art_assessment_model/microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at ./art/art_assessment_model/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at ./art/art_assessment_model/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
loadingg model performence {'design': {'acc': 1.0, 'srcc': 0.2663765360725705, 'plcc': 0.3104542940974841, 'rmse': 5.15143, 'mae': 4.426708}, 'technology': {'acc': 1.0, 'srcc': -0.5364910866637292, 'plcc': -0.7353007716855294, 'rmse': 8.328009, 'mae': 7.9323983}, 'market': {'acc': 1.0, 'srcc': 0.3040862677512799, 'plcc': 0.13234074539940338, 'rmse': 5.2439446, 'mae': 4.6281013}, 'investment': {'acc': 1.0, 'srcc': 0.26594604029325725, 'plcc': 0.07640204066079584, 'rmse': 6.752481, 'mae': 6.003188}, 'media': {'acc': 1.0, 'srcc': 0.377810656959309, 'plcc': 0.27712845862496166, 'rmse': 5.6284695, 'mae': 4.8510404}}
loadingg model performence {'design': {'acc': 1.0, 'srcc': 0.2663765360725705, 'plcc': 0.3104542940974841, 'rmse': 5.15143, 'mae': 4.426708}, 'technology': {'acc': 1.0, 'srcc': -0.5364910866637292, 'plcc': -0.7353007716855294, 'rmse': 8.328009, 'mae': 7.9323983}, 'market': {'acc': 1.0, 'srcc': 0.3040862677512799, 'plcc': 0.13234074539940338, 'rmse': 5.2439446, 'mae': 4.6281013}, 'investment': {'acc': 1.0, 'srcc': 0.26594604029325725, 'plcc': 0.07640204066079584, 'rmse': 6.752481, 'mae': 6.003188}, 'media': {'acc': 1.0, 'srcc': 0.377810656959309, 'plcc': 0.27712845862496166, 'rmse': 5.6284695, 'mae': 4.8510404}}
preLoadAllProduct
httpMIddleware-get <QueryDict: {}>
